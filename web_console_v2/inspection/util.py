# Copyright 2023 The FedLearner Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import enum
from typing import Optional
import json
import logging

import fsspec
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.types import StructType
from urllib.parse import urlparse

EXAMPLE_ID = 'example_id'
DEFAULT_SCHEME_TYPE = 'file'


class FileSuffixError(Exception):

    def __init__(self, message):
        super().__init__()
        self.message = message

    def __repr__(self):
        return f'{type(self).__name__}({self.message})'


def dataset_schema_path(dataset_path: str) -> str:
    return os.path.join(dataset_path, 'schema.json')


def build_spark_conf(conf: Optional[SparkConf] = None) -> SparkConf:
    """Builds spark config by following our practices."""
    if not conf:
        conf = SparkConf()

    # Ref: https://spark.apache.org/docs/3.1.1/configuration.html

    # ---------- speculation related
    # Re-launches tasks if they are running slowly in a stage
    conf.set('spark.speculation', 'true')
    # Checks tasks to speculate every 100ms
    conf.set('spark.speculation.interval', 100)
    # Fraction of tasks which must be complete before speculation is enabled for a particular stage.
    conf.set('spark.speculation.quantile', 0.9)
    # How many times slower a task is than the median to be considered for speculation.
    conf.set('spark.speculation.multiplier', 2)
    # ---------- end of speculation related

    # disable compression to snappy as model training not support reading snappy file extension
    conf.set('spark.hadoop.mapred.output.compress', 'false')

    return conf


def getenv(name: str, default: str = None) -> str:
    value = os.getenv(name)
    if value is not None:
        return value
    if default is None:
        raise ValueError(f'Environment variable {name} is not set')
    return default


def load_tfrecords(spark: SparkSession, files: str, dataset_path: str) -> DataFrame:
    logging.info(f'### loading df..., input files path: {files}')
    # read schema
    try:
        with fsspec.open(dataset_schema_path(dataset_path)) as f:
            schema = StructType.fromJson(json.load(f))
        return spark.read.format('tfrecords').schema(schema).load(files)
    except Exception as e:  # pylint: disable=broad-except
        # intersection dataset generated by FLApp has no schema, ignore it
        logging.info(f'### failed to load dataset schema, err: {e}')
        return spark.read.format('tfrecords').load(files)


def is_file_matched(path: str) -> bool:
    fs: fsspec.AbstractFileSystem = fsspec.get_mapper(path).fs
    glob_path = path
    # input path of spark might be dir, file or wildcard, while fsspec glob could only check file and wildcard
    # so we manually add wildcard ** to dir
    if fs.isdir(path):
        glob_path = os.path.join(path, '**')
    files = fs.glob(glob_path)
    # ignore _SUCCESS file and xxx._SUCCESS file
    data_files = [file for file in files if os.path.split(file)[1] != '_SUCCESS' and not file.endswith('._SUCCESS')]
    return len(data_files) > 0


class FileFormat(str, enum.Enum):
    CSV = 'csv'
    TFRECORDS = 'tfrecords'
    UNKNOWN = 'unknown'


def load_by_file_format(spark: SparkSession, input_batch_path: str, file_format: FileFormat) -> DataFrame:
    if file_format == FileFormat.CSV:
        return spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(input_batch_path)
    if file_format == FileFormat.TFRECORDS:
        return spark.read.format('tfrecords').load(input_batch_path)
    err_msg = f'### no valid file format, format: {file_format}'
    logging.error(err_msg)
    raise ValueError(err_msg)


def normalize_file_path(url: Optional[str]) -> Optional[str]:
    if url is None:
        return url
    url_parser = urlparse(url)
    scheme = url_parser.scheme
    # set default source_type if no source_type found
    if scheme == '' and url.startswith('/'):
        url = f'{DEFAULT_SCHEME_TYPE}://{url}'
    return url
